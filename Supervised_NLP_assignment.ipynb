{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from nltk.corpus import gutenberg, stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A model was given to us that identified features of a sentences from alice in wonderland and persuasion. Our target was to identify the author of the sentence by using these features. Try to improve model by adding more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(r'\\[.*\\]', \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice[:int(len(alice)/3)])\n",
    "persuasion = text_cleaner(persuasion[:int(len(persuasion)/3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_sents = [[sent, 'Caroll'] for sent in list(alice_doc.sents)]\n",
    "persuasion_sents = [[sent, 'Austen'] for sent in list(persuasion_doc.sents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns=['sent', 'author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    all_words = [token.lemma_ for token in text if token.is_punct == False and token.is_stop == False]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(all_words).most_common(2000) if item[1] > 1]\n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame()\n",
    "    df['sentence'] = sentences['sent']\n",
    "    \n",
    "    def sent_length(s):\n",
    "        return len(s['sentence'])\n",
    "    \n",
    "    df['sent_length'] = df.apply(sent_length, axis=1)\n",
    "    \n",
    "    def punct(s):\n",
    "        punct = [token.text for token in s['sentence']\n",
    "                 if token.is_punct == True]\n",
    "        return Counter(punct)\n",
    "        \n",
    "    df['punct'] = df.apply(punct, axis=1)\n",
    "    \n",
    "    def lemma(s):\n",
    "        words = [token.lemma_ for token in s['sentence']\n",
    "                 if token.is_punct == False\n",
    "                 and token.is_stop == False\n",
    "                 and token.lemma_ in common_words]\n",
    "        return Counter(words)\n",
    "    \n",
    "    df['lemmas'] = df.apply(lemma, axis=1)\n",
    "    \n",
    "    def pos(s):\n",
    "        pos = [token.pos_ for token in s['sentence'] if token.is_punct == False and token.is_stop == False]\n",
    "        return Counter(pos)\n",
    "    \n",
    "    df['pos'] = df.apply(pos, axis=1)\n",
    "    \n",
    "    def dep(s):\n",
    "        dep = [token.dep_ for token in s['sentence'] if token.is_punct == False and token.is_stop == False]\n",
    "        return Counter(dep)\n",
    "    \n",
    "    df['dependencies'] = df.apply(dep, axis=1)\n",
    "    \n",
    "    for index in df.index:\n",
    "        words = [x for x in df.loc[index, 'lemmas']]\n",
    "        for word in words:\n",
    "            df.loc[index, 'word_' + word] = df.loc[index, 'lemmas'][word]\n",
    "\n",
    "        pos = [x for x in df.loc[index, 'pos']]\n",
    "        for part in pos:\n",
    "            df.loc[index, 'pos_' + part] = df.loc[index, 'pos'][part]\n",
    "\n",
    "        puncts = [x for x in df.loc[index, 'punct']]\n",
    "        for punctuation in puncts:\n",
    "            df.loc[index, 'punct_' + punctuation] = df.loc[index, 'punct'][punctuation]\n",
    "            \n",
    "        denpendencies = [x for x in df.loc[index, 'dependencies']]\n",
    "        for depend in denpendencies:\n",
    "            df.loc[index, 'dep_' + depend] = df.loc[index, 'dependencies'][depend]\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def postive_sentiment(s):\n",
    "        return analyzer.polarity_scores(s['sentence'].text)['pos']\n",
    "\n",
    "    def negative_sentiment(s):\n",
    "        return analyzer.polarity_scores(s['sentence'].text)['neg']\n",
    "\n",
    "    def compound_sentiment(s):\n",
    "        return analyzer.polarity_scores(s['sentence'].text)['compound']\n",
    "    \n",
    "    df['postive_sentiment'] = df.apply(postive_sentiment, axis=1)\n",
    "    df['negative_sentiment'] = df.apply(negative_sentiment, axis=1)\n",
    "    df['compound_sentiment'] = df.apply(compound_sentiment, axis=1)\n",
    "    \n",
    "                \n",
    "    df['text_source'] = sentences['author']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "df = bow_features(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(int(0), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_source(s):\n",
    "    if s['text_source'] == 'Caroll':\n",
    "        return 0\n",
    "    if s['text_source'] == 'Austen':\n",
    "        return 1\n",
    "    \n",
    "df['author'] = df.apply(text_source, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(['author', 'text_source', 'sentence', 'lemmas', 'pos', 'punct', 'dependencies'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['author']\n",
    "X = features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9952107279693486\n",
      "\n",
      "Test set score: 0.9008620689655172\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9636015325670498\n",
      "\n",
      "Test set score: 0.9181034482758621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "lr.fit(X_train, y_train)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out whether your new model is good at identifying Alice in Wonderland vs any other work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'\\[.*\\]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "alice = re.sub(r'\\*', '', alice)\n",
    "hamlet = re.sub(r'Actus.*\\.', '', hamlet)\n",
    "hamlet = re.sub(r'Scoena.*\\.', '', hamlet)\n",
    "\n",
    "hamlet = text_cleaner(hamlet[:int(len(hamlet)/3)])\n",
    "alice = text_cleaner(alice[:int(len(alice)/3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_doc = nlp(hamlet)\n",
    "alice_doc = nlp(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_sents = [[sent, 'Shakespear'] for sent in list(hamlet_doc.sents)]\n",
    "alice_sents = [[sent, 'Carroll'] for sent in list(alice_doc.sents)]\n",
    "\n",
    "all_sents = hamlet_sents + alice_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_authors = pd.DataFrame(all_sents, columns=['sentence', 'author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW(text):\n",
    "    \n",
    "    all_words = [token.lemma_ for token in text if token.is_punct == False and token.is_stop == False]\n",
    "    \n",
    "    return [item[0] for item in Counter(all_words).most_common(2000) if item[1] > 1]\n",
    "\n",
    "hamlet_words = BoW(hamlet_doc)\n",
    "alice_words = BoW(alice_doc)\n",
    "\n",
    "all_words = set(hamlet_words + alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW_features(sentences, all_words):\n",
    "    df = pd.DataFrame()\n",
    "    df['sentence'] = sentences['sentence']\n",
    "    \n",
    "    def words(s):\n",
    "        words = [token.lemma_ for token in s['sentence']\n",
    "                 if token.is_punct == False\n",
    "                 and token.is_stop == False\n",
    "                 and token.lemma_ in all_words]\n",
    "        return Counter(words)\n",
    "    \n",
    "    def pos(s):\n",
    "        pos = [token.pos_ for token in s['sentence'] if token.is_punct == False and token.is_stop == False]\n",
    "        return Counter(pos)\n",
    "    \n",
    "    def dep(s):\n",
    "        dep = [token.dep_ for token in s['sentence'] if token.is_punct == False and token.is_stop == False]\n",
    "        return Counter(dep)\n",
    "    \n",
    "    def punct(s):\n",
    "        punct = [token.text for token in s['sentence'] if token.is_punct == True]\n",
    "        return Counter(punct)\n",
    "    \n",
    "    df['sent_length'] = df['sentence'].apply(lambda x: len(x))\n",
    "    df['words'] = df.apply(words, axis=1)\n",
    "    df['parts_of_speech'] = df.apply(pos, axis=1)\n",
    "    df['dependencies'] = df.apply(dep, axis=1)\n",
    "    df['punctuation'] = df.apply(punct, axis=1)\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df['sentiment'] = df['sentence'].apply(lambda x: analyzer.polarity_scores(x.text)['compound'])\n",
    "    \n",
    "    for index in df.index:\n",
    "        \n",
    "        words = [x for x in df.loc[index, 'words']]\n",
    "        for word in words:\n",
    "            df.loc[index, word + '_count'] = df.loc[index, 'words'][word]\n",
    "        \n",
    "        pos = [x for x in df.loc[index, 'parts_of_speech']]\n",
    "        for part in pos:\n",
    "            df.loc[index, part + '_pos'] = df.loc[index, 'parts_of_speech'][part]\n",
    "        \n",
    "        dep = [x for x in df.loc[index, 'dependencies']]\n",
    "        for depend in dep:\n",
    "            df.loc[index, depend + '_dep'] = df.loc[index, 'dependencies'][depend]\n",
    "        \n",
    "        punct = [x for x in df.loc[index, 'punctuation']]\n",
    "        for punc in punct:\n",
    "            df.loc[index, punc + '_punct'] = df.loc[index, 'punctuation'][punc]\n",
    "    \n",
    "    df['author'] = sentences['author'].apply(lambda x: 1 if x == 'Carroll' else 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = BoW_features(sentences_authors, all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(['author', 'sentence', 'words', 'parts_of_speech', 'punctuation', 'dependencies'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['author']\n",
    "X = features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9766970618034447\n",
      "\n",
      "Test set score: 0.939209726443769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "lr.fit(X_train, y_train)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9645390070921985\n",
      "\n",
      "Test set score: 0.9224924012158054\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier(n_estimators=500, max_depth=16)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
